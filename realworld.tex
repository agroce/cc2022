\section{Non-experimental Fuzzing Campaigns}
\label{real-world}

\begin{table}
\caption{Fuzzing campaign results for real world bugs. {
\cmark~is \textbf{fixed} bugs. \clock~is \textbf{confirmed but unfixed bugs}. 
\acirc~is \textbf{duplicate bug reports}.
\textbf{Total} is the number of true, unique bugs reported and acknowledged.}
}
\centering
\begin{tabular}{lrr|rrr}
\toprule
                    \bf Project       & \bf Length & \bf Total                        & \cmark            & \clock                  & \acirc                 \\
\midrule
                    Solidity          & 20mo 30d      & \solUniqueFixedOrConfirmed      & \solUniqueFixed   & \solUniqueConfirmed     & \solAValidDuplicates   \\
                                      % Fuzz: about a week Reports: Jan 27 to Feb 9 2021.
                    Move              & 20d        & \movUniqueFixedOrConfirmed       & \movUniqueFixed   & \movUniqueConfirmed     & 0                      \\
                                                                                                                                    % 63 submittd, 49 are fixed or confirmed, are the other 14 wrong or dups?
                    Fe                & 9mo 6d      & \feUniqueFixedOrConfirmed        & \feUniqueFixed    & \feUniqueConfirmed      & \feValidDuplicates \\
                                      % Fuzz: about a week. Reports: 1 day
                    Zig               & 7d         & \zigUniqueFixedOrConfirmed       & \zigUniqueFixed   & \zigUniqueConfirmed     & 0                      \\
\midrule
                    All               &            & \allUniqueFixedOrConfirmed       & \allUniqueFixed   & \allUniqueConfirmed     & \allValidDuplicates    \\
\bottomrule
\end{tabular}

\label{tab:campaign-fixes}
\end{table}

In addition to our controlled experiments fuzzing a version of each compiler before we reported any bugs, we ran real fuzzing campaigns, updating the compiler versions as new commits were made, and adjusting our corpus to include new tests, of various durations, on each compiler.  Two of these campaigns are ongoing (for Solidity and Fe) and have been well-supported and well-received by the compiler teams.

\begin{sloppypar}
  Perhaps the most important evidence of the real world effectiveness is that we fuzzed the Solidity compiler for over a year with our approach, and in that time reported a large number of otherwise unreported bugs that have been fixed; we submitted our most recent bug (as of this writing) on 11/2/21.  Prior to and during our campaign, Solidity had been fuzzed heavily using AFL, using a dictionary, by the developers and by external contributors, and has been on OSS-Fuzz since the first quarter of 2019 (\url{https://blog.soliditylang.org/2021/02/10/an-introduction-to-soliditys-fuzz-testing-approach/}).  While no grammar-based approach has been applied to Solidity as a whole, the Yul IL has been fuzzed using Google's libprotobuf-mutator library (\url{https://github.com/google/libprotobuf-mutator}).  Despite competing with these efforts, and never devoting more resources to the fuzzing than 3-4 docker container hosted instances of our fuzzing tool, running on a high-end laptop, we believe that our campaign was the largest single source of fuzzing-discovered bugs in the compiler during our campaign.  The campaign was awarded a security bounty of \$1,000 USD in Ethereum for discovery of a bug with potential security implications (\url{https://github.com/ethereum/solidity/issues/8368}) (and, it was noted, for the general effectiveness of the fuzzing), and the Solidity team encouraged and aided our efforts, once it was clear that the approach was very useful in exposing subtle bugs not otherwise discovered.  Because Solidity bug triage is very well supported, we can add an additional measure of the effectiveness of our approach in finding bugs that involve ``realistic'' code.  After October of 2020, the Solidity team began adding a ``should compile without error'' label to submitted bugs that involved legal code the compiler rejects.  Of the 38 bugs we submitted since that date, 15 (nearly 40\%) have involved correct but rejected (via a crash) code.  Such bugs are inherently harder to find and usually more interesting than those where a compiler crashes rather than reporting an error when given invalid code.
\end{sloppypar}

A second long-term fuzzing effort was directed at the Fe language, a Rust/Python-like alternative to Solidity for writing Ethereum contracts.  Fe is an experimental language, and the project has far fewer resources than Solidity to devote to testing.  Fe developers received this effort warmly, and quickly made some changes to the Fe compiler to make AFL fuzzing more effective (by crashing when Fe caused the Yul backend to fail).  Using our approach, we were able to provide the project with high-quality fuzzing very early in the lifetime of an experimental compiler project.  We speculate that better ``no-fuss'' fuzzing could expose language corner cases early in the implementation of a compiler, avoiding having to make costly changes later, when more code depends on erroneous implementation assumptions or (even more disastrously) poor language design choices.  Some of our bug reports triggered lengthy discussions in the issue of a language or compiler design foundational decision.  Due to the small size of the Fe team, 8 of our reported bugs have not been analyzed yet and confirmed as valid.  The most recent of these was submitted (as of this writing) on 9/25/21.  The Fe effort was sufficiently influential that it was invoked in discussions of the long-term strategies for building language-customized fuzzing for Fe (e.g., \url{https://github.com/ethereum/fe/pull/578#pullrequestreview-790913799}).

At the time we fuzzed Facebook's \texttt{Move} compiler, the project had been fuzzing various components, but less so the compiler itself.
The majority of bugs reported were quickly confirmed and fixed, and developers expressed interest in incorporating our approach into CI (\url{https://github.com/diem/diem/issues/7384#issuecomment-769443728}).


We ran a shorter, less-intensive campaign on the \texttt{Zig} compiler.  The
\texttt{Zig} compiler continues to be under heavy development, and a small team
of maintainers are prioritizing efforts to rewrite the components where we found
bugs.

\begin{sloppypar}
  We additionally reported 5 (mostly parser crash) bugs in the SPIN model checker, which were all fixed (e.g., \url{https://github.com/nimble-code/Spin/commit/7f364a1b174f08e9ede49e342f411e209af26a84}).
  \end{sloppypar}
