\section{Introduction}

Compilers are notoriously hard to test, and modern optimizing
compilers tend to contain many subtle bugs.  Compiler bugs can have
serious consequences, including potentially the introduction of
security vulnerabilities that cannot be detected by human or static
analysis without knowledge of the compiler flaw~\cite{CompBug}.   The
literature on compiler testing is extensive~\cite{chen2020survey} and
goes back to such foundational papers as McKeeman's introduction of
the idea of differential testing in the context of random testing of
compilers~\cite{Differential}.

As McKeeman's work suggests, one core approach to testing compilers is
based on
the generation of random programs.  In recent
years, the Csmith~\cite{csmith} project is perhaps the most prominent
example of this method.  However, builiding a tool such as Csmith
is a heroic effort, requiring considerable expertise and
development time.  Csmith itself is over 30KLOC, much of it complex
and with a lengthy development history.  Csmith is focused on a
single, albeit extremely important, language, C.  Building a tool like
Csmith for a new programming language is not within the scope of most
language or compiler development projects, even very prominent ones.  Even for such a highly
visible language/compiler project as Rust/rustc, to our knowledge
there is \emph{no} useful tool for generating random Rust programs
(and none seems to be
used in {\tt rustc} testing).  As far as we can tell, Rust
is primarily (or perhaps \emph{only}) fuzzed at the whole language
level
(\url{https://github.com/dwrensha/fuzz-rustc/blob/master/fuzz_target.rs}) by
using a wrapper around libFuzzer, a tool with no special knowledge of Rust
syntax or semantics, to randomly modify \emph{a set of supplied Rust
programs}.  Similarly, the {\tt solc} compiler, which essentially
defines the Solidity language used to write most smart contracts for
the Ethereum blockchain, is not fuzzed using a Csmith-like
generator, but using methods similar to those used for
Rust, again based on taking
existing Solidity programs and randomly modifying them.

This approach (randomly mutating existing programs) is widely used by
real-world compiler projects in part because \emph{it is often very easy to
  apply}: we call if ``no fuss'' compiler fuzzing.  Most compiler projects, even large ones, do not have a team
of random testing experts available, so the construction of
Csmith-like tools is out of the question.  This means that the only
way to generate valid programs \emph{from scratch} is to use a tool that takes as
input the \emph{grammar} of a language and generates random outputs
satisfying the grammar.   However, such an approach has multiple problems.
First, in many cases the programs produced by a grammar, without
extensive attention to tuning the tool, will be mostly uninteresting.
Csmith is successful in part because of the use of numerous heuristics
to generate interesting code.  Second, the grammar of a language alone
seldom provides guidance in avoiding simple errors that cause programs
to be rejected without exploring interesting compiler behavior; a BNF
grammar generally will not, for example, force identifiers to be
defined before they are used.  Third, many interesting bugs can only
be exposed by programs that may not satisfy a language's grammar, in
theory, due to differences between a formal grammar and the actual
parser used in a compiler, or other subtle implementation details.
Salls et al.~\cite{Salls2021TokenLevel} elaborate this weakness in a
recent paper, finding that many bugs could not be found using a
grammar-based generator.
Finally, a usable grammar may not be available, especially as the
tools will expect a grammar in a particular format, and may add
restrictions on the structure of the grammarr.  In the early stages,
many programming language projects lack a stable, well-defined
grammar in any formal, standalone, notation.  An ad-hoc ``grammar'' used by the compiler implementation may be the
only grammar around.  Thus, while grammar-based compiler
testing has sometimes been extremely successful (e.g., the
LangFuzz~\cite{LangFuzz} approach), few compilers are actually
extensively tested using purely grammar-based tools.

Therefore, many projects rely on an approach we will define in more
detail below, which uses off-the-shelf \emph{fuzzing} tools,
originally designed to find security vulnerabilities in programs, to
modify existing inputs (programs) rather than generate programs from
scratch.  This approach has found many subtle compiler bugs, but
suffers from two major drawbacks:

\begin{enumerate}
\item First, in most cases the methods used by such programs to modify
  (mutate) inputs tend to take code that exercises interesting
  compiler behavior, and transform it into code that is rejected in
  the early stages of parsing.  That is, in most cases,
  general-purpose fuzzers tend to take code and turn it into
  ``non-code.''  This makes such fuzzing inefficient, and makes it
  almost impossible for it to find bugs requiring many subtle
  modifications of corpus programs.
  \item Second, when such fuzzers do find bugs, the bugs are often
    found in particularly un-humanlike inputs, such as code
    containing non-printable bytes.  Bugs are often at the ``crash the
    parser'' level rather than deeper semantic levels of the compiler.
  \end{enumerate}

  Combined together, these problems make most compiler fuzzing
  performed in practice, even on major projects, both inefficient in
  terms of finding bugs and perhaps prone to find bugs that are not
  the most important and interesting compiler bugs.

  Given that this kind of ``no-fuss'' fuzzing is the \emph{only} option available in
  practice to small compiler projects that cannot devote resources to
  tuning a grammar-based fuzzer (or perhaps even supporting an
  always-up-to-date grammar), improving the effectiveness of such
  compiler fuzzing is an obvious way to practically improve compiler
  testing.  Ideally, such improvements would not require \emph{any}
  additional effort or change the workflow of existing compiler
  fuzzing setups (other than changing the  tool to be used).

  This paper proposes one such improvement, based on changing the way
  in which general-purpose fuzzers modify (mutate) inputs.  We augment
  the set of changes made by such tools with a large number of
  modifications drawn from the domain of \emph{mutation testing},
  which only modifies code in ways likely to preserve desirable
  properties, such as the ability to get through a parser.  We evaluate
  our technique on several real-world compilers, and show that it
  dramatically improves the mean number of distinct compiler bugs
  detected, and moreover produces a much larger set of distinct,
  successfully compiling, inputs that explore compiler behavior.  As a
  result of our approach, which is available as an easy-to-use tool
  based on the widely used AFL fuzzer, we have reported more than 100
  bugs in important real-world compilers, the great majority of
  which have been confirmed and fixed, and received a bug bounty for
  our efforts.  In the longest-running campaign, that targeting the
  {\tt solc} compiler for Solidity code, we were the first to
  report a very large number of serious bugs, despite continuous, extensive
  fuzzing using un-modified AFL performed by the compiler developers
  and external contributors, over the same time frame.