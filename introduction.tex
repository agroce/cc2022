\section{Introduction}

Compilers are notoriously hard to test, and modern optimizing
compilers tend to contain many subtle bugs.  Compiler bugs can have
serious consequences, including, potentially, the introduction of
security vulnerabilities that cannot be detected without knowledge of a compiler flaw~\cite{CompBug}.   The
literature on compiler testing is extensive~\cite{chen2020survey}.

\small
\begin{table*}
\centering
\begin{tabular}{p{35mm}p{31mm}p{30mm}p{67mm}}
\toprule
\bf Technique & \bf Tool & \bf Requirements    & \bf Weaknesses \\
              &          & \bf from Developers &                \\
% \midrule
\rowcolor{LLGray}
Custom tool (e.g. Csmith)  
& Custom tool 
& None 
& Extremely labor-intensive, potentially years of work
\\
%\hline
Grammar-based              
& Grammar-based fuzzer             
& Usable grammar 
& Needs tuning, many bugs not in scope 
\\
% \hline
\rowcolor{LLGray}
``No-fuss'' mutation-based 
& Off-the-shelf fuzzer \newline (e.g., AFL) 
& Corpus of examples 
& Inefficient, has trouble hitting ``deep'' bugs; may focus on ``bad'' code that should not compile but causes crashes 
  \\
\bottomrule
\end{tabular}
\caption{Compiler Fuzzing Techniques}
\label{tab:techniques}
\end{table*}
\normalsize

As McKeeman's~\cite{Differential} widely cited paper suggests, one core approach to testing compilers is
based on
the generation of \emph{random programs}.  The Csmith~\cite{csmith} project is perhaps the most prominent
example of this method.  Builiding a tool such as Csmith
is a heroic effort, requiring considerable expertise and
development time.  Csmith itself is over 30KLOC, much of it complex
and with a lengthy development history.  Csmith is focused on a
single, albeit extremely important, language: C.  Building a tool like
Csmith for a new programming language is not within the scope of most
compiler projects, even major ones.  For instance, to our knowledge
there is \emph{no} useful tool for generating random Rust programs
(certainly none seems to be
prominently featured in {\tt rustc} testing).  As far as we can tell, Rust
is primarily (or perhaps \emph{only}) fuzzed at the whole language
level
(\url{https://github.com/dwrensha/fuzz-rustc/blob/master/fuzz_target.rs}) by
using a wrapper around libFuzzer, a tool with no special knowledge of Rust
syntax or semantics, to randomly modify \emph{a set of supplied Rust
programs}.  Similarly, the {\tt solc} compiler, used for most smart contracts on
the Ethereum blockchain, is not fuzzed using a Csmith-like
generator, but mostly using methods similar to those used for
Rust.  Creating a
grammar-based fuzzer has been an open issue for Solidity since August
of 2020 (\url{https://github.com/ethereum/solidity/issues/9673}).

\begin{sloppypar}
Generating compiler tests by randomly mutating existing programs is widely used by
real-world compiler projects in part because \emph{it is often very easy to
  apply}: we call it ``no-fuss'' compiler fuzzing.  Most compiler projects, even large ones, do not have a team
of spare random testing and compiler/language experts available, so the construction of
Csmith-like tools is out of the question.  This means that the only
way to generate valid programs \emph{from scratch} is to use a tool that takes as
input the \emph{grammar} of a language and generates random outputs
satisfying the grammar.   However, such an approach has multiple problems.
First, in many cases the programs produced by a grammar, without
extensive attention to tuning the probabilities of productions, etc., will be mostly uninteresting.
Csmith is successful in part because of the use of numerous heuristics
to generate interesting code.  Second, the grammar of a language alone
seldom provides guidance in avoiding simple errors that cause programs
to be rejected without exploring interesting compiler behavior; e.g., forcing identifiers to be
defined before they are used.  Third, many interesting bugs can \emph{only}
be exposed by programs that do not satisfy a language's supposed grammar, due to differences between a formal grammar and the actual
parser used in a compiler, or other subtle implementation details.
Salls et al.~\cite{Salls2021TokenLevel} found that many bugs could not be discovered using a
grammar-based generator.
Finally, a usable grammar simply may not be available, especially as the
tools will expect a grammar in a particular format (e.g. antlr4), and may add
restrictions on the structure of the grammar.  In the early stages,
many programming language projects lack a stable, well-defined
grammar in any formal, standalone, notation.  An ad-hoc ``grammar'' used by the compiler implementation may be the
only grammar around.  Thus, while grammar-based compiler
testing has sometimes been extremely successful~\cite{LangFuzz}, few compilers are actually
extensively tested using grammar-based tools.
\end{sloppypar}

Unfortunately, ``no-fuss''
fuzzing must make use of off-the-shelf \emph{fuzzing} tools,
originally designed to find security vulnerabilities in programs, thatessentially treat inputs as an
undifferentiated byte-sequence, with little or no structure.  No-fuzz
fuzzing has found many subtle compiler bugs, but
suffers from two major drawbacks:

\begin{enumerate}
\item The methods used by fuzzers to mutate inputs tend to take code that exercises interesting
  compiler behavior, and transform it into code that is rejected by
  the parser.  This is inefficient, and makes it
  almost impossible to find bugs requiring a sequence of subtle
  modifications.
  \item Second, when such fuzzers do find bugs, the bugs are often
    found in particularly un-humanlike inputs, often invalid programs.
  \end{enumerate}

  Combined together, these problems make most compiler fuzzing
  performed in practice, even on major projects, both inefficient in
  terms of finding bugs and perhaps prone to find bugs that are not
  the most important and interesting compiler bugs.
  Table~\ref{tab:techniques} summarizes the existing widely-used
  compiler fuzzing techniques and their weaknesses.

  Given that ``no-fuss'' fuzzing is widely used in large projects and
  may be the \emph{only} option available in
  practice to small compiler projects, improving the effectiveness of no-fuss fuzzing is an obvious way to practically improve compiler
  testing.  Ideally, such improvements would not require \emph{any}
  additional effort or change the workflow of existing compiler
  fuzzing setups.

  This paper proposes one such improvement, based on changing the way
  in which general-purpose fuzzers modify (mutate) inputs.  We augment
  the set of primarily byte-based changes made by such tools with a large number of
  modifications drawn from the domain of \emph{mutation testing},
  which only modifies code in ways likely to preserve desirable
  properties --- such as the ability to get through a parser.  We evaluate
  our technique on four real-world compilers, and show that it
  significantly improves the mean number of distinct compiler bugs
  detected.  As a
  result of our approach, which is available as an easy-to-use tool
  based on the widely used AFL fuzzer
  (\url{https://github.com/google/AFL}), we have reported more than
  100 previously undiscovered 
  bugs, subsequently fixed, bugs in important real-world compilers, and received a bug bounty for
  our efforts.  In the longest-running campaign, that targeting the
  {\tt solc} compiler for Solidity code, we were the first to
  report a large number of serious bugs, despite extensive
  fuzzing using AFL performed by the compiler developers
  and external contributors, over the same time frame, including
  OSS-fuzz continuous fuzzing.
