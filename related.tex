\section{Related Work}

Research on compiler testing, as noted in the introduction, has been an important subfield overlapping compiler development and design and software engineering and testing, for many years.  Chen et al. summarize much of this work in a recent survey~\cite{chen2020survey}.

To our knowledge, very little work has appeared targeting the problem this paper addresses: improving the ability of general-purpose fuzzers to find (interesting) bugs in compilers.  The recent work of Salls et al.~\cite{Salls2021TokenLevel}, however, specifically aims to improve general-purpose fuzzer performance on compilers and interpreters.  Their approach, which they call ``token-level fuzzing'' essentially produces a hybrid level in between grammar-based generation and ``byte-level'' mutation-based fuzzing.  The core of their idea is to replace the largely byte-level mutations of AFL etc. with mutations at the \emph{token} level of a grammar.  They summarize the idea as ``valid tokens should be replaced with valid tokens''~\cite{Salls2021TokenLevel}.  In a sense, this extends the idea of using a dictionary, but with important changes:  token-level fuzzing \emph{only} applies token-level, not byte-level mutations, but also adds the composition of multiple token additions and substitutions to the set of single-step mutations.  Token-level fuzzing is an attractive and useful idea, somewhat orthogonal to our approach.  However, unlike our approach, token-level fuzzing does not apply AFL's havoc operations, so some bugs are simply not possible to find using token-level fuzzing (e.g., ones involving injecting unprintable characters in strings, including our Solidity bug earning a security bounty).  In this sense, token-level fuzzing has some of the limitations of grammar-based generation.  Token-level fuzzing also provides little help to a fuzzer in deleting large chunks of code, since this often would require a very large number of token operations, though the approach does include a way to copy statements from one input to another.  Finally, token-level fuzzing requires using a lexer to find all tokens in input seeds, and if tokens not in those seeds would be useful, developers must provide any additional tokens.  This requires modifying the fuzzing workflow to add token pre-processing, and is no longer strictly \emph{no-fuss}, though in practice the change is fairly small (which is also true of our approach when comby pre-processess the corpus).