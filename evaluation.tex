\section{Evaluation}
\label{eval}

We ran a series of controlled experiments to evaluate the effectiveness of two
new strategies based on our approaches in Sections~\ref{strat-fast-string-level}
and~\ref{strat-syntax-aware} for improving on ``no-fuss'' compiler fuzzing. Our
main goal is to answer to what extent these low-effort strategies demonstrate
significant benefit in the domain of compiler fuzzing, and how they influence
fuzzer behavior and performance.

Section~\ref{exp-setup} describes our experimental setup.
Section~\ref{exp-results} summarizes our results, which compares our strategies
against stock AFL and AFL++ fuzzers on four actively-developed compilers. 

\subsection{Experimental Setup}
\label{exp-setup}

We evaluated our approach using four compilers,  for the languages Solidity,\footnote{https://docs.soliditylang.org/}
Move,\footnote{https://move-book.com/} and Fe\footnote{https://fe-lang.org/} (smart contract languages)
and the trending general-purpose language
Zig.\footnote{https://ziglang.org}.  Move and Fe are implemented in
Rust, Solidity is implemented in C++, and Zig is implemented in a mix
of C++ and Zig itself.  Solidity is a very widely used language, since
it is the primary language used for smart contracts on the Ethereum
blockchain.  Move and Fe are experimental smart contract languages,
and Zig is an up-and-coming systems language operating in
the same space as C, Rust, Nim, and other statically-typed languges
with manual memory management.

\noindent \textbf{Fuzzer configurations.} We perform a comparative evaluation
of four fuzzing configurations.  The first configuration is stock \texttt{AFL},
our baseline of comparison across all projects. Because our approach is
integrated directly into stock \texttt{AFL}, we're comparing to the baseline
\emph{implementation} (a desirable practice for sound comparative fuzzer
evaluations~\cite{BoehmeCR21}).
%Citation says: > If possible, the proposed technique (e.g., an improvement to
%gray > box fuzzing) is implemented directly > into the baseline (e.g., AFL).
Although \texttt{AFL} continues to be a de facto industry standard for
``no-fuss'' fuzzing, numerous community-driven improvements have been made to
the \texttt{AFL++} project, which can often outperform stock \texttt{AFL}.
Therefore, for extra measure, we sought to compare our results to a second
fuzzing configuration using the existing \texttt{AFL++} tool. We were
successful at configuring \texttt{AFL++} for three of our four compilers
(excluding Zig), and report our results in Section~\ref{exp-results}.
Note, however, that since our technique is not implemented in \texttt{AFL++}, the
comparison is incongruent, and potentially handicapped by orthogonal
\texttt{AFL++} improvements that may stand to boost our
approach.\footnote{Indeed, recent improvements in \texttt{AFL++}, not available
at the time of our first implementation, compel us to consider the
reimplemnting our approach in \texttt{AFL++}.}

The third and fourth configurations we compare are both strategies based on our
new approaches in Section~\ref{implementation}. The third configuration applies
purely string-level mutations as described in
Section~\ref{strat-fast-string-level} with \textbf{75\%} probability of
attempting to mutate the input. The fourth configuration augments the pure
string-level mutation strategy with syntax-aware mutation
(Section~\ref{strat-syntax-aware}), where our \texttt{AFL} has \textbf{33\%}
probability to request that the server generate a new input (using template
splicing), \textbf{33\%} probability to perform string-level mutation on the
input, and \textbf{34\%} probability to run \texttt{AFL} as usual.

We chose the ratios in our strategies with a best-effort method by running just
a single 24 hour trial on a single project (Solidity) for various
configurations (e.g., 90\% string-level mutation, 75\% pure syntax-aware
mutation, and 25\%-50\% ratio). We found the 75\% and 33\%-33\% strategies
to be the best candidates to evaluate deeply over many hours of fuzzing. We
note the especially appealing avenue of future work for devising optimal
selections of these parameters (perhaps based on language attributes, or input
corpora).

\noindent \textbf{Fuzzing trials and duration.} We ran 16 trials per fuzzing
configuration for each compiler to control for variability and randomness.  We
ran four configurations for the Solidity, Move, and Fe compilers (where
\texttt{AFL++} is included), and three configurations for Zig (\texttt{AFL++})
excluded) for a total of 112 trials.  A single trial comprises 24 hours of
fuzzing on a single core, starting from the initial input corpus.  %
16*3*24+16*4*24 = 2,688 hours We chose 24 hour trials because our intent is to
answer whether we can observe (relatively immediate) effects of strategies that
aim to surface deeper bugs.  Our choice aligns with existing work that shows
finding new vulnerabilities earlier during a fuzzing campaign is proportionally
cheaper (more likely) than long-running campaigns~\cite{fuzzexp}. I.e., if our
strategies exhibit any significant competitive advantage, we expect it
to manifest
early for a controlled setting (within 24 hours).  In aggregate, our
experiments represent 112 days of fuzzing to demonstrate fuzzer performance for
quickly surfacing bugs with our ``no-fuss'' enhancements on these compilers.
Each project was fuzzed at an early commit before we had reported bugs to the
upstream repository.

\noindent 
\textbf{Input corpora.} All fuzzer trials ran over inputs derived from the
project's own source tree. A summary is shown in Table~\ref{tab:inputs}. For
example, the Solidity base corpus is 2,447 \textbf{Files} ending in
\texttt{.sol} in the \texttt{test/libsolidity} subdirectory.  For fuzzer trials
using syntax-aware input generation, we decompose the base corpus into unique
templates (\textbf{Templ.}) and concrete program fragments (\textbf{Frag.}).
Because this process can yield very large (and therefore slow) inputs during
generation, we remove all templates and fragments larger than 4KB.  For
Solidity, the base corpus decomposes into 9,308 templates and 7,651 concrete
program fragments.  The remaining project corpora are as follows:

\begin{table}[h!]
\centering
\begin{tabular}{llrrr}
\toprule
                    \bf Proj          & \bf Source                    & \bf Files         & \bf Templ.     & \bf Frag. \\
\midrule
                    \mr{1}{Solidity}  & \texttt{.sol} test files      & 2,447             & 9,308         & 7,651     \\
                    \mr{1}{Move}      & all \texttt{.diem}            & 1,103             & 9,650         & 10,916    \\
                    \mr{1}{Fe}        & all \texttt{.fe}              & 126               & 253           & 153          \\
                    \mr{1}{Zig}       & compile-error tests           & 586               & 1,762         & 1,562      \\ 
\bottomrule
\end{tabular}
\caption{Summary of input corpora for four compilers.}
\label{tab:inputs}
\end{table}
\vspace{-3em}
{\color{red} TODO Rijnard: come back and mention average
time to generate templates/fragments. It's somewhere in the range of 15 mins to
an hour per project}. 

For fuzzer trials using the 33\% syntax-aware mutation strategy, we start fuzzing
with an empty, ``zero'' program and rely entirely on the probbility to
introduce newly generated inputs from the \textbf{Templ.} and \textbf{Frag.}
over time. This gives ample opportunity to potentially generate new
combinations of inputs at runtime without processing seed inputs. All other
configurations start with the seed \textbf{Files}, which \texttt{AFL} processes
to discover ones with ``interesting'' coverage properties.

\noindent \textbf{Hardware.} Each trial ran on Ubuntu 18.04, on a single core
of Intel Xeon Gold 6240 2.6 GHz CPUs, and with up to 30GB free RAM.

% Data lives here:
% Solidity: /home/rijnard/0-experiments-feb/sol-programs                         @ 686b62b585d686f08fe2f8d586b8474c133dce2f + cherry-pick
%           /home/rijnard/0-experiments-feb/comby-mutation-server/fragments
% Move:     /home/rijnard/0-experiments-feb/move-programs                        @ bfb6b09715894b3c436919bf2e718b6ae0fcba9f (double check)
%           /home/rijnard/0-experiments-feb/comby-mutation-server-move/fragments
% FE        /home/rijnard/0-experiments-feb/fe-programs                          @ 1ea2206e3d10e77163f1a01bee05088358d8ef23
%           /home/rijnard/0-experiments-feb/comby-mutation-server-fe/fragments


\subsection{Results}
\label{exp-results}


\begin{table*}[t!]
\centering
\begin{tabular}{llrrrrrrc}
\toprule
                    \bf Project      & \bf Configuration
  & \mc{3}{c}{\bf Unique Bugs}        & \bf Avg Execs  & \bf Avg Paths
  & \bf Avg Bitmap    & Compiles (K)    \\
                                     &                                             & Avg     & Min       & Max         & (Millions)     & (K)              & Cvg (\%)          &   \\
\midrule
                    \mr{4}{Solidity} & \tt \small      AFL-baseline                &  3.69   & 1         &  6          & 35.8           & 12.0             & 54.34\ph{a}       & 2.89                     \\ 
                                     & \tt \small      AFL++ 3.15a                 &  5.63   & 1         & 10          & 56.9           &  8.8             & 20.58$^\dagger$   & 3.80                 \\ 
                                     & \tt \small      text-mutation               &  7.81   & 7         & 11          & 30.3           & 14.3             & 55.65\ph{a}       & 5.48                \\ 
                                     & \tt \small      splice-mutation             & 11.81   & 7         & 14          & 16.0           & 16.8             & 57.33\ph{a}       & 5.24                 \\ 
\midrule
                    \mr{4}{Move}     & \tt \small      AFL-basline                 & 7.19    & 6         & 8           & 56.9           & 4.9              & 63.23\ph{a}       &                             \\ 
                                     & \tt \small      AFL++ 2.54b                 & ????    & ?         & ?           & ????           & ???              & ?????             &                          \\ 
                                     & \tt \small      text-mutation               & 8.31    & 7         & 9           & 61.2           & 6.0              & 62.27\ph{a}       &                         \\ 
                                     & \tt \small      splice-mutation             & 6.06    & 5         & 7           &  7.2           & 5.0              & 63.18\ph{a}       &                         \\ 
\midrule
                    \mr{4}{Fe}       & \tt \small      AFL-baseline                & 6.56    & 5         & 7           & 24.5           & 3.5              & 27.91\ph{a}       &                          \\ 
                                     & \tt \small      AFL++ 2.64c                 & 6.44    & 5         & 8           & 22.6           & 3.4              & 27.76\ph{a}       &                       \\ 
                                     & \tt \small      text-mutation               & 6.50    & 5         & 7           & 17.9           & 3.3              & 27.84\ph{a}       &                          \\ 
                                     & \tt \small      splice-mutation             & 6.94    & 6         & 9           &  5.0           & 2.6              & 27.83\ph{a}       &                          \\ 
\midrule
                    \mr{3}{Zig}      & \tt \small      AFL-baseline                & ????    & ?         & ??          & 2.2            & 3.3              & 40.99\ph{a}       &                         \\ 
                                     & \tt \small      text-mutation               & ????    & ?         & ??          & 2.1            & 3.3              & 40.95\ph{a}       &                       \\ 
                                     & \tt \small      splice-mutation             & ????    & ?         & ??          & 1.3            & 3.9              & 41.82\ph{a}       &                        \\ 
\bottomrule
\end{tabular} 
        \caption{Main results of controlled experiment. We fuzzed each project for 16 trials (24 hours per trial) in different configurations: \texttt{baseline-AFL}, \texttt{AF++},  \texttt{text-mutation}, and \texttt{splice-mutation}.
\texttt{baseline-AFL} is stock \texttt{AFL}; \texttt{AFL++} is a
community-driven effort that enhances stock
AFL. \texttt{text-mutation} applies fast string-based mutation operators (textual
find-replace patterns) with a probability of 75\% on every fuzzed
input. Stock AFL manipulates the input the remaining 25\% of the
time. \texttt{splice-mutation} is a hybrid approach that (1) applies
mutation operators as in \texttt{text-mutation} with probability 33\%;
(2) synthesizes a syntax-aware input with template (splice) with
probability 33\%; and (3) uses stock AFL the remaining 34\% of the time.}
\label{tab:results}
\end{table*}

Our main result is that our enhancements with string-level and syntax-aware
mutation consistently uncover unique bugs missed by \texttt{AFL} and
\texttt{AFL++}, often performing better and yielding a higher overall discovery
of unique bugs. Which strategy is favorable varies per project, and in some
cases, one of our strategies underperforms due to the ``fast or smart?''
tradeoff. We first give an overview of results followed by notable
thematic observations.

\noindent \textbf{Overview.} Table~\ref{tab:results} summarizes the fuzzing
runs for each project and configuration pair. A row in the table represents the
average number over 16 trials for a project in that configuration.
\texttt{AFL-baseline} is stock \texttt{AFL}'s results. For projects Solidity,
Move, and Fe, we include \texttt{AFL++}'s results. The \texttt{AFL++} version
varies based on the version compatible with a project's commit at which we
started fuzzing. All \textbf{Unique Bugs} are determined by classifying crashing
inputs by bug-fixing commits (we assume a single commit fixes a single bug, an
assumption that is consistent with prior
work~\cite{semantic-crash-bucketing,Taming}, general software development
practice, and our manual inspection). I.e., \textbf{Unique bugs} is a precise
ground truth of truly unique bugs.

In general, either our \texttt{text-mutation} or \texttt{splice-mutation}, or
both, does better than existing tools. Our strongest result shows that the
hybrid \texttt{splice-mutation} strategy discovers roughly 8 more bugs than
\texttt{AFL} ($\approx3\times$ as many) on Solidity on average. For Solidity,
the simple approach of \texttt{text-mutation} also performs well, discovering
roughly 4 more bugs than AFL. Especially interesting, the predictability of
\texttt{AFL} and \texttt{AFL++} bug discovery on Solidity varies highly,
sometimes just finding a single unique bug (Min = 1) in a trial. The root cause
is that both these fuzzers get ``stuck'' exploring a parser
bug,\footnote{Commit
\href{https://github.com/ethereum/solidity/commit/0b9c842656c644c209280e5f570f94dee77a1606}{0b9c84}
in \texttt{github.com/ethereum/solidity.}} thinking that every new crash is
interesting. Our approaches didn't fall prey to this pathological behavior, and
always found at least 7 unique bugs, which is likely accounted for by
variability in input mutation. Our weakest result reveals two related insights:

\begin{itemize}
\item One of our strategies may not perform better than stock approaches. This is the case for Fe, where \\ \texttt{text-mutation} performs only on par with \texttt{AFL}. Here, \texttt{splice-mutation} is the only strategy to perform slightly better than other tools.
\item One of our strategies may underperform compared to stock approaches. This is the case for Move, where \texttt{splice-mutation} performs worse than the baseline, \\ whereas \texttt{text-mutation} performs best.
\end{itemize}

\begin{sloppypar}
These observations highlight the potential \emph{tradeoffs} of ``fast or
smart?'' mutation. In the former case, simple mutations are not enough to
enrich the search space and discover more bugs, but the smarter hybrid variety
does perform marginally better, despite being more than 3 times \emph{slower}
than all other approaches by average number of executions (\textbf{Avg.
Execs}). Conversely, the quick \texttt{text-mutation} approach does best in the
Move project, where the \texttt{splice-mutation} hybrid variety is just too
slow ($\approx8\times$ slower) to compensate for its ``smart''
benefit.
\end{sloppypar}

\textbf{A look at quick and exclusive findings.} Perhaps the most compelling
case for choosing ``fast or smart'' mutation enhancements is the ability to
discover bugs that are exclusive to a particular strategy (requiring a rather
unique alteration or combination of inputs to find), or to identify unique,
not-so-shallow bugs \emph{quickly}. In the former case, for instance, our results
showed that the slightly better bug discovery of \texttt{splice-mutation} in Fe
could be attributed to a \emph{consistent} discovery of a Rust borrow error
(triggered in all 16 trials) that was never discovered by any other
tool.\footnote{Commit
\href{https://github.com/ethereum/fe/commit/3b977b3078eb163ba521f57d8509e16efdb9dbf4}
3b977b in \texttt{github.com/ethereum/fe.}} In the latter case, we see that the
hybrid approach on Solidity consistently discovers a particular contract
bug\footnote{Report ommitted for anonymity} while it is never discovered by the
\texttt{text-mutation} strategy. Yet notably, during the process of running long
term campaigns (Section~\ref{real-world}), this same bug \emph{was} eventually discovered by the pure
\texttt{text-mutation} approach after some period longer than 24 hours. In
these cases, evidence points to hybrid approaches leading to rapid and
exclusive discoveries of certain bugs arising from 
the combination of simple and complex mutations on the input space.

While finding only 1 additional bug or 1.5 additional bugs may, at
first, seem like a modest gain, it is important to recall that in
fuzzing, finding additional bugs is \emph{extremely hard}.  B\"{o}hme
and Falk~\cite{fuzzexp} show that ``[W]ith twice the machines, we can
find \emph{all known} bugs in half the time. Yet, finding linearly
\emph{more} bugs in the same time requires exponentially more
machines'' \cite{fuzzexp}.  That is, finding even one more bug may be
very costly.  Moreover, as our real-world results below show, over
time the ability to detect additional bugs adds up to a substantial
number of new bugs detected, even for a compiler being aggressively
fuzzed by numerous other techniques, using more computing power.

\textbf{Properties of mutated inputs.} For Solidity, we also examined the distinct number of \emph{compiling}
inputs in the final queue (the set of all interesting, non-crashing,
inputs AFL produced).  We investigated this because the number of
paths found is somewhat uninformative for compilers, where in general
paths that expose peculiar parse errors are less interesting for
testing the compiler's internals than paths involving different
behavior in the stages of compilation after parsing.  The most serious
compiler bugs, with potential to produce \emph{wrong code}, break
invariants in the later stages of compilation, especially during
complex optimizations.  We therefore looked
at the number of interesting inputs (interesting because of new
coverage of some kind) that \emph{actually compiled} as a rough
approximation of how much behavior triggering deeper stages of
compilation the fuzzing strategies found, and this number is reported in the
final column of Table~\ref{tab:results}.  On average, both of our approaches
found thousands more such inputs than AFL stock or AFL++.

