\section{Evaluation}
\label{eval}

We ran a a controlled experiment to evaluate the effectiveness of two
new strategies based on our approaches in
Sections~\ref{strat-fast-string-level} and~\ref{strat-syntax-aware} to
improve ``no-fuss'' compiler fuzzing. Our main goal is to answer to what
extent these low-effort strategies demonstrate significant benefit in the
domain of compiler fuzzing, and how they influence fuzzer behavior and
performance.

%Section~\ref{exp-setup} describes our experimental setup.
%Section~\ref{exp-results} summarizes our results, which compares our strategies
%against stock \texttt{AFL} and \texttt{AFL++} fuzzers on four actively-developed compilers. 

\subsection{Experimental Setup}
\label{exp-setup}

\begin{sloppypar}
We evaluate our approach on four compilers, for the languages
Solidity,
Move, Fe
and
Zig\footnote{See
        \href{https://docs.soliditylang.org}{soliditylang.org},
        \href{https://move-book.com}{move-book.com},
        \href{https://fe-lang.org}{fe-lang.org} and
        \href{https://ziglang.org}{ziglang.org}.}.
Solidity is a high-profile language for writing smart contracts on the Ethereum
blockchain, and very widely used. Fe
is an experimental statically-typed language for Ethereum smart
contracts. Move is Facebook's smart contract language, developed for the
industrial blockchain solution Diem. Zig
is an up-and-coming systems language operating in the same space as C, Rust,
Nim, and other statically-typed languages with manual memory management.
Move and Fe are implemented in Rust, Solidity is implemented in C++, and Zig is
implemented in a mix of C++ and Zig itself.
\end{sloppypar}

\noindent \textbf{Fuzzer configurations.} We perform a comparative evaluation
of four fuzzing configurations.  The first configuration is stock \texttt{AFL}
in "quick \& dirty" mode, our baseline of comparison across all projects.
Because our approach is integrated directly into stock \texttt{AFL}, we're
comparing to the baseline \emph{implementation} (a desirable practice for sound
comparative fuzzer evaluations~\cite{BoehmeCR21}).
%Citation says: > If possible, the proposed technique (e.g., an improvement to
%gray > box fuzzing) is implemented directly > into the baseline (e.g., AFL).
Although \texttt{AFL} continues to be a de facto industry standard for
``no-fuss'' fuzzing, numerous community-driven improvements have been made to
the \texttt{AFL++} project, which can often outperform stock \texttt{AFL}.
Therefore, for extra measure, we seek to compare our results to a second
fuzzing configuration using the existing \texttt{AFL++} tool. We successfully
configured \texttt{AFL++} for three of our four compilers (excluding Zig), and
report our results in Section~\ref{exp-results}.  Note, however, that since our
technique is not implemented in \texttt{AFL++}, the comparison is incongruent,
and potentially handicapped by orthogonal \texttt{AFL++} improvements that may
stand to boost our approach.\footnote{Indeed, recent improvements, not
available during our first implementation, compel us to implement our approach
in \texttt{AFL++}.}

The third and fourth configurations we compare are both strategies based on our
new approaches in Section~\ref{implementation}. The third configuration applies
purely string-level mutations as described in
Section~\ref{strat-fast-string-level} with \textbf{75\%} probability. The fourth configuration augments the pure
string-level mutation strategy with syntax-aware mutation
(Section~\ref{strat-syntax-aware}), where our \texttt{AFL} has \textbf{33\%}
probability to request that the server generate a new input (using template
splicing), \textbf{33\%} probability to perform string-level mutation on the
input, and \textbf{34\%} probability to run \texttt{AFL} as usual.

We chose the ratios in our strategies with a best-effort method by running just
a single 24 hour trial on a single project (Solidity) for various
configurations (e.g., 90\% string-level mutation, 75\% pure syntax-aware
mutation, and 25\%-50\% ratio). We found the 75\% and 33\%-33\% strategies
to be the best candidates to evaluate deeply over many hours of fuzzing. We
note the especially appealing avenue of future work for devising optimal
selections of these parameters (perhaps based on language attributes, or input
corpora).

\noindent \textbf{Fuzzing trials and duration.} We ran 14 trials per fuzzing
configuration for each compiler to control for variability and randomness.  We
ran four configurations for the Solidity, Move, and Fe compilers (where
\texttt{AFL++} is included), and three configurations for Zig (\texttt{AFL++}
excluded) for a total of 210 trials.  A single trial comprises 24 hours of
fuzzing on a single core, starting from the initial input corpus.  
% 14*3*24+14*4*24 = 2,352
We chose 24 hour trials because our intent is to
answer whether we can observe (relatively immediate) effects of strategies that
aim to surface deeper bugs.  Our choice aligns with existing work that shows
finding new vulnerabilities earlier during a fuzzing campaign is proportionally
cheaper (more likely) than long-running campaigns~\cite{fuzzexp}. If our
strategies exhibit any significant competitive advantage, we expect it
to manifest
early in a controlled setting (within 24 hours).  In aggregate, our
experiments represent 210 days of fuzzing to demonstrate fuzzer performance for
quickly surfacing bugs with our ``no-fuss'' enhancements on these compilers.
Each project was fuzzed at an early commit before we had reported bugs to the
upstream repository.

\begin{sloppypar}
\noindent 
\textbf{Input corpora and preprocessing} All fuzzer trials ran over inputs derived from the
project's own source tree. A summary is shown in Table~\ref{tab:inputs}. For
example, the Solidity base corpus is 2,447 \textbf{Files} ending in
\texttt{.sol} in the \texttt{test/libsolidity} subdirectory.  For fuzzer trials
using syntax-aware input generation, we decompose the base corpus into unique
templates (\textbf{Templ.}) and concrete program fragments (\textbf{Frag.}).
Because this process can yield very large (and therefore slow) inputs during
generation, we remove all templates and fragments larger than 4KB.  For
Solidity, the base corpus decomposes into 9,308 templates and 7,651 concrete
program fragments.  The other corpora are:
\end{sloppypar}

{\scriptsize
\begin{table}[h!]
\centering
\begin{tabular}{llrrr}
\toprule
                    \bf Proj          & \bf Source                    & \bf Files         & \bf Templ.     & \bf Frag. \\
\midrule
                    \mr{1}{Solidity}  & \texttt{.sol} test files      & 2,447             & 9,308         & 7,651     \\
                    \mr{1}{Move}      & all \texttt{.move}            & 1,103             & 9,650         & 10,916    \\
                    \mr{1}{Fe}        & all \texttt{.fe}              & 127               & 253           & 153          \\
                    \mr{1}{Zig}       & compile-error tests           & 586               & 1,762         & 1,562      \\ 
\bottomrule
\end{tabular}
\caption{Summary of input corpora for four compilers.}
\label{tab:inputs}
\end{table}
\vspace{-1em}
% ./extract.sh .sol .go  3425.20s user 1130.89s system 110% cpu 1:08:47.10 total
% ./extract.sh .move .rs  6679.28s user 1923.79s system 102% cpu 2:19:58.64 total
% ./extract.sh .fe .generic  82.36s user 29.65s system 112% cpu    1:39.89 total
% ./extract.sh .zig .zig  1632.26s user 549.67s system 110% cpu 32:52.66 total
% average: 60.5
}

Creating \textbf{Templ.} and \textbf{Frag.} input components incurs some
preprocessing per project. This time ranges from less than 2 minutes (for Zig)
to 2 hours (for Move). Our setup does not attempt to adjust the 24 hour fuzzing
time to compensate for this preprocessing, because in a real world setting this
(relatively small) preprocessing cost  is greatly amortized over the time of a
real fuzzing campaign that typically lasts much longer than 24 hours.  Even in
our experimental setup, this cost, which would account for for less than 5\% of
the fuzzing time on average for a single trial, is amortized over multiple 24
hour trials, and thus difficult to adjust for fairly. A benefit of this up-front
per-project process is that our fuzzer trials using the 33\% syntax-aware
mutation strategy start fuzzing with an empty, ``zero'' program and rely
entirely on the probability to potentially generate new combinations inputs
from the \textbf{Templ.} and \textbf{Frag.} at runtime. This is architecturally
distinct from our other \texttt{AFL} fuzzing configurations (both the baseline
and pure string-level mutations) that run preprocessing bundled with
AFL, filtering initial inputs (\textbf{Files}) based on code coverage (which may
take seconds to minutes, or even longer, depending on corpus size)
before fuzzing begins.
In brief, preprocessing times in our configurations are not directly
comparable, but it is reasonable to assume that the costs for all
configurations converge to zero in realistic, long-running campaigns.

\noindent \textbf{Hardware.} Each trial ran on Ubuntu 18.04, on a single core
of Intel Xeon Gold 6240 2.6 GHz CPUs, with ~30GB free RAM.

% Data lives here:
% Solidity: /home/rijnard/0-experiments-feb/sol-programs                         @ 686b62b585d686f08fe2f8d586b8474c133dce2f + cherry-pick
%           /home/rijnard/0-experiments-feb/comby-mutation-server/fragments
% Move:     /home/rijnard/0-experiments-feb/move-programs                        @ bfb6b09715894b3c436919bf2e718b6ae0fcba9f (double check)
%           /home/rijnard/0-experiments-feb/comby-mutation-server-move/fragments
% FE        /home/rijnard/0-experiments-feb/fe-programs                          @ 1ea2206e3d10e77163f1a01bee05088358d8ef23
%           /home/rijnard/0-experiments-feb/comby-mutation-server-fe/fragments

 
\subsection{Results}
\label{exp-results}


\begin{table*}[t!]
\centering
\begin{tabular}{llrrrrrrcc}
\toprule
                    \bf Project      & \bf Configuration
  & \mc{3}{c}{\bf Unique Bugs}        & \bf Avg Execs  & \bf Avg Paths
        & \bf Avg Bitmap    & \mc{2}{l}{\bf Compiles}  \\
                                     &                                             & Avg     & Min       & Max         & (Millions)     & (K)              & Cvg (\%)          &  (K) & (\%) \\
\midrule
                    \mr{4}{Solidity} & \tt \small      AFL-baseline                &  3.79   & 1         &  6          & 35.6           & 12.1             & 54.32\ph{a}       & 2.94 & 20.2              \\ 
                                     & \tt \small      AFL++ 3.15a                 &  5.21   & 1         &  8          & 57.9           &  8.8             & 20.59$^\dagger$   & 3.76 & 33.4          \\ 
                                     & \tt \small      text-mutation               &  7.57   & 7         &  9          & 30.3           & 14.2             & 55.65\ph{a}       & 5.48 & 32.7         \\ 
                                     & \tt \small      splice-mutation             & 11.79   & 7         & 14          & 16.0           & 16.8             & 57.32\ph{a}       & 5.24 & 31.1          \\ 
\midrule
                    \mr{4}{Move}     & \tt \small      AFL-baseline                & 7.14    & 6         & 8           & 56.3           & 4.9              & 63.21\ph{a}       & 1.77 & 29.5                 \\ 
                                     & \tt \small      AFL++ 2.64c                 & 6.36    & 5         & 7           & 47.7           & 4.5              & 62.43\ph{a}       & 1.63 & 28.7              \\ 
                                     & \tt \small      text-mutation               & 8.43    & 7         & 9           & 61.5           & 6.0              & 63.28\ph{a}       & 2.38 & 33.2             \\ 
                                     & \tt \small      splice-mutation             & 6.00    & 5         & 8           &  7.2           & 5.0              & 63.16\ph{a}       & 1.21 & 23.8             \\ 
\midrule
                    \mr{4}{Fe}       & \tt \small      AFL-baseline                & 6.57    & 5         & 7           & 24.3           & 3.5              & 27.93\ph{a}       & 0.55 & 14.8              \\ 
                                     & \tt \small      AFL++ 2.64c                 & 6.50    & 5         & 8           & 22.8           & 3.4              & 27.76\ph{a}       & 0.48 & 13.2           \\ 
                                     & \tt \small      text-mutation               & 6.50    & 5         & 7           & 17.9           & 3.3              & 27.84\ph{a}       & 0.47 & 13.5              \\ 
                                     & \tt \small      splice-mutation             & 6.93    & 6         & 9           &  6.0           & 2.6              & 27.84\ph{a}       & 0.42 & 15.1              \\ 
\midrule
                    \mr{3}{Zig}      & \tt \small      AFL-baseline                & 2.57    & 1         & 5           & 2.2            & 3.4              & 40.99\ph{a}       & 0.12 &  3.2             \\ 
                                     & \tt \small      text-mutation               & 2.36    & 0         & 4           & 2.1            & 3.3              & 40.96\ph{a}       & 0.13 &  3.3           \\ 
                                     & \tt \small      splice-mutation             & 7.64    & 0         & 13          & 1.3            & 3.9              & 41.84\ph{a}       & 0.27 &  7.1            \\ 
\bottomrule
\end{tabular} 
        \caption{Main results of controlled experiment. {\scriptsize We fuzzed each project for 14 trials (24 hours per trial) in different configurations: \texttt{baseline-AFL}, \texttt{AFL++},  \texttt{text-mutation}, and \texttt{splice-mutation}.
\texttt{baseline-AFL} is stock \texttt{AFL}; \texttt{AFL++} is a
community-driven effort that enhances stock
AFL. \texttt{text-mutation} applies fast string-based mutation operators (textual
find-replace patterns) with a probability of 75\% on every fuzzed
input. Stock AFL manipulates the input the remaining 25\% of the
time. \texttt{splice-mutation} is a hybrid approach that (1) applies
mutation operators as in \texttt{text-mutation} with probability 33\%;
(2) synthesizes a syntax-aware input with template (splice) with
        probability 33\%; and (3) uses stock AFL the remaining 34\% of the time. 
        $\dagger$The instrumentation for a more recent version
        \texttt{3.15a} of \texttt{AFL++} differs from our baseline
        version.}
}
\label{tab:results}
\end{table*}

Our main result is that our enhancements with string-level and syntax-aware
mutation consistently uncover unique bugs missed by \texttt{AFL} and
\texttt{AFL++}, often performing better and yielding a higher overall discovery
of unique bugs. Which strategy is favorable varies per project, and in some
cases, one of our strategies underperforms due to the ``fast or smart?''
tradeoff. We first give an overview of results followed by notable
thematic observations.

\noindent \textbf{Overview.} Table~\ref{tab:results} summarizes the fuzzing
runs for each project and configuration pair. A row in the table represents the
average number over 14 trials for a project in that configuration.
\texttt{AFL-baseline} is stock \texttt{AFL}'s results. For projects Solidity,
Move, and Fe, we include \texttt{AFL++}'s results. The \texttt{AFL++} version
varies based on the version compatible with a project's commit at which we
started fuzzing. All \textbf{Unique Bugs} for Solidity, Move, and Fe are
determined by classifying crashing inputs by bug-fixing commits, except for at
most 2 unique, yet unfixed Solidity bugs manually inspected by an expert
author. We assume a single commit fixes a single bug, an assumption that is
consistent with prior work~\cite{semantic-crash-bucketing,Taming}, general
software development practice, and our manual inspection, and thus \textbf{unique
bugs} is a precise ground truth of truly unique bugs. \textbf{Unique Bugs} for
Zig are largely unfixed at the time of writing, and are instead comprehensively
classified by an expert author manually inspecting every crashing input and its
error message. Not reported in the table, a proportion of (potentially
duplicative) crashing inputs for \texttt{Move} were found by all configurations
that remain untriaged, because we could not find a bug-fixing commit, or
because no fix exists yet.

\begin{sloppypar}
In general, either our \texttt{text-mutation} or \texttt{splice-mutation}, or
both, does better than existing tools. One of our strongest results shows that the
hybrid \texttt{splice-mutation} strategy discovers roughly 8 more bugs than
\texttt{AFL} (approximately 3$\times$ as many) on Solidity on average. For Solidity,
the simple approach of \texttt{text-mutation} also performs well, discovering
roughly 4 more bugs than AFL. Especially interesting, the predictability of
\texttt{AFL} and \texttt{AFL++} bug discovery on Solidity varies highly,
sometimes just finding a single unique bug (Min = 1) in a trial. The root cause
is that both these fuzzers get ``stuck'' exploring a parser
bug, (see 
\url{https://github.com/ethereum/solidity/commit/0b9c842656c644c209280e5f570f94dee77a1606})
thinking that every new crash is
interesting. Our approaches didn't fall prey to this pathological behavior, and
always found at least 7 unique bugs, which is likely accounted for by
variability in input mutation. Another strong result is the discovery of more than $2\times$
as many unique bugs found via \texttt{splice-mutation} in Zig compared to other approaches.
Somewhat similar to Solidity, \texttt{splice-mutation} on Zig overcomes
a pathological behavior where usual \texttt{AFL} fuzzing finds
extremely large inputs ``interesting'', but they do not actually crash
the compiler. This behavior affects all configurations (accounting for 0 unique bugs for some trials with \texttt{text-mutation} and \texttt{splice-mutation}) but is effectively suppressed on average by variation created with \texttt{splice-mutation}. We expand more on the qualitative appeal of our approaches below.
On the whole, our weakest result reveals two related
insights:
\end{sloppypar}

\begin{itemize}
\item One of our strategies may not perform better than stock approaches. This is the case for Fe, where \\ \texttt{text-mutation} performs only on par with \texttt{AFL}. Here, \texttt{splice-mutation} is the only strategy to perform slightly better than other tools.
\item One of our strategies may underperform compared to stock approaches. This is the case for Move, where \texttt{splice-mutation} performs worse than the baseline, \\ whereas \texttt{text-mutation} performs best.
\end{itemize}

\begin{sloppypar}
These observations highlight the potential \emph{tradeoffs} of ``fast or
smart?'' mutation. In the former case, simple mutations are not enough to
enrich the search space and discover more bugs, but the smarter hybrid variety
does perform marginally better, despite being more than 3 times \emph{slower}
than all other approaches by average number of executions (\textbf{Avg.
Execs}). Conversely, the quick \texttt{text-mutation} approach does best in the
Move project, where the \texttt{splice-mutation} hybrid variety is just too
slow (almost 8$\times$ slower) to compensate for its ``smart''
benefit.
\end{sloppypar}

\textbf{A look at quick and exclusive findings.} With exception to Fe, the
improvement of our best approach for each project is statistically significant
(by Wilcoxon rank-sum test, $p < 0.05$).  But rather than a statistical
measure, which treats every unique bug as equal, perhaps the most compelling
case for choosing ``fast or smart'' mutation enhancements lies in the
qualitative properties of our approaches. Specifically, we found that our
approaches tend to discover bugs that are exclusive to a particular strategy
(requiring a rather unique alteration of inputs to find), or to identify
unique, not-so-shallow bugs \emph{quickly}. In the former case, for instance,
looking into qualitative results of Fe revealed that the slightly better bug
discovery of \texttt{splice-mutation} could be attributed to a
\emph{consistent} discovery of a Rust borrow error (triggered in all 14 trials)
that was never discovered by any other tool (see
\url{https://github.com/ethereum/fe/commit/3b977b3078eb163ba521f57d8509e16efdb9dbf4}). In the latter case, we see that the
hybrid approach on Solidity consistently discovers a particular contract
bug that is never discovered by the
\texttt{text-mutation} strategy. Yet notably, during the process of running
long term campaigns (Section~\ref{real-world}), this same bug \emph{was}
eventually discovered by the pure \texttt{text-mutation} approach after some
period longer than 24 hours. As another strong qualitative measure, our
\texttt{splice-mutation} approach \emph{exclusively} found a majority of 14
unique bugs out of a total 25 unique bugs across all approaches and trials.  In
these cases, evidence points to hybrid approaches leading to rapid and
exclusive discoveries of unique bugs arising from the combination of simple and
complex mutations.

Additionally, while finding only 1 additional bug or 1.5 additional bugs in some cases may,
at first, seem like a modest gain, it is important to recall that in fuzzing,
finding additional bugs is \emph{extremely hard}.  B\"{o}hme and
Falk~\cite{fuzzexp} show that ``[W]ith twice the machines, we can find
\emph{all known} bugs in half the time. Yet, finding linearly \emph{more} bugs
in the same time requires exponentially more machines'' \cite{fuzzexp}.  That
is, finding even one more bug may be very costly.  Moreover, as our real-world
results in Section~\ref{real-world} show, over time the ability to detect
additional bugs adds up to a substantial number of new bugs detected, even for
a compiler being aggressively fuzzed by numerous other techniques, using more
computing power.

\textbf{Properties of mutated inputs.} We also examined the distinct number of
\emph{compiling} inputs in the final queue (the set of all interesting,
non-crashing, inputs AFL produced).  We investigated this because the number of
paths found (cf. \textbf{Avg Paths}, Table~\ref{tab:results}) is somewhat
uninformative for compilers, wherein general paths that expose peculiar parse
errors are less interesting for testing the compiler's internals than paths
involving different behavior in the stages of compilation after parsing.  The
most serious compiler bugs, with potential to produce \emph{wrong code}, break
invariants in the later stages of compilation, especially during complex
optimizations.  We therefore looked at the number and percentage of interesting
inputs (interesting because of new coverage of some kind) that \emph{actually
compiled} (\textbf{Compiles} in Table~\ref{tab:results}) as a rough
approximation of how much behavior triggering deeper stages of compilation the
fuzzing strategies found. On average, our approaches tend to do better either
in the absolute number of compiling bugs (K) or percentage of compiling bugs
(\%) when the values of other approaches are steady. For example, our approaches
find thousands more such inputs in Solidity, while the percentage of compiling
bugs of the total set varies little compared to \texttt{AFL++}. On the other hand,
looking at the best-performing bug finder on Fe, \texttt{splice-mutation}
generates a similar number of compilable programs to other tools, but with a slightly
higher ratio. It is also interesting to take this number into consideration
with the number of executions, particularly for the \texttt{splice-mutation}
strategy. The relationship suggests that the ``fast or smart'' tradeoff surfaces in terms of
compilable programs during fuzzing, i.e., chosen strategies may boost the
absolute number or percentage of compiling programs and yield more
bugs or higher quality bugs.
