\section{Mutation-Testing-Based Compiler Fuzzing}



\subsection{Mutation-Based Fuzzing}

One use of the term ``mutation'' appears in the context of \emph{mutation-based} fuzzing~\cite{ArtFuzz}, the primary random testing approach used by many compiler projects, as discussed above.  Again, we note that there are two basic kinds of compiler testing based on the generation of random inputs to a compiler.  One, in recent years paradigmatically expressed in the Csmith tool~\cite{csmith}, works by using a grammar and/or deep knowledge of the language accepted by the compiler, to generate progams to test the compiler.  This is sometimes called \emph{generative} fuzzing.  Generative fuzzing can be very effective, but often requires expert tuning of a large, sophisticated tool, and at minimum requires having a suitable usable grammar for the language of the compiler.  In practice, many compiler projects do not employ generative fuzzing for practical reasons.

A second approach, and the only approach widely used in many major compiler projects, is to use an off-the-shelf fuzzer, such as is used to find security vulnerabilities (e.g., the ubiquitous American Fuzzy Lop (afl) \url{https://github.com/google/AFL}) or libFuzzer, and a \emph{corpus} of example programs, such as the set of regression tests for the compiler or a set of real-world programs.  A fuzzer such as AFL operates by executing the program under test (here, the compiler) on inputs (initially those in the corpus), using instrumentation to determine code coverage in the compiler for each executed input.  The fuzzer then takes inputs that look interesting and adds them to a \emph{queue}.  The basic loop is then to take some input from the queue, \emph{mutate} it by making some essentially random change (e.g., flipping a single bit, or removing a random chunk of bytes), execute the new, mutated input under instrumentation, and add the new input to the queue if it seems ``interesting'' --- typically, if it hits some kind of coverage target that has not previously been hit.  The details of selecting inputs from the queue and determining how to mutate an input vary widely, and improving the effectiveness of this basic approach has been a major topic of recent software testing and security research.  However, the basic strategy usually still fits into a simple basic model:

\begin{enumerate}
\item Select an input from the queue.
\item Mutate that input in order to obtain a new input.
\item Execute the new input, and if it is deemed interesting, add it to the queue.
\item Go back to the first step.
\end{enumerate}

Any inputs that crash the compiler in step 3 are reported to the user.  Using such a fuzzer is often extremely easy, involving no more work than 1) building the compiler with special instrumentation and 2) finding a set of initial programs to use as a corpus.  Even compiling with instrumentation can be optional; some fuzzers (including AFL) can use QEMU to fuzz arbitrary binaries.  However, for compilers, it is usually best if possible to rebuild the compiler, since QEMU-based execution is much slower, and compilers are slow enough to seriously degrade fuzzing throughput.


Our work focuses on improving step 2 of this process, in a way that is agnostic to how the details of the other aspects of fuzzing are implemented.  In particular, the problem with most approaches to mutation in the literature, for compiler fuzzing, is that changes such as byte-level-transformations almost always take compiling programs that exercise interesting compiler behavior, and transform them into programs that don't make it past early stages of parsing.  Alternative approaches to what are called ``havoc''-style mutations tend to involve solving constraints or following taint, which in the case of compilers tends to be ineffective, since the relationships to be preserved are quite complex, and implemented in complex code.  A second common approach, providing a \emph{dictionary} of meaningful byte sequences in a language, is both burdensome on compiler developers (though less so than providing a full grammar), and limited in effectiveness: a dictionary cannot, for example, help the fuzzer delete meaningful sub-units of code, such as statements or blocks.

We propose a novel way to produce a much larger number of useful, interesting mutations for source code, without paying an analytical price that makes fuzzing practically infeasible for compilers, and without requiring \emph{any} additional effort on the part of compiler developers.

\subsection{Mutation Testing}

A different use of the term ``mutation'' appears in the field of mutation testing.  Mutation testing~\cite{MutationSurvey,budd1979mutation,demillo1978hints} is an approach to evaluating and improving software tests that works by introducing small syntactic changes into a program, under the assumption that if the original program was correct, then a program with slightly different semantics will be incorrect, and should be flagged as such by effective tests.  Mutation testing is now widely used in software testing research, and is used to varying degrees in industry at-scale and for especially critical software development~\cite{mutKernel,mutGoogle,mutFacebook}.

A mutation testing approach is defined by a set of mutation operations.  Such operations vary widely in the literature, though a few, such as deleting a small portion of code (such as a statement) or replacing arithmetic and relational operations (e.g., changing {\tt +} to {\tt -} or {\tt ==} to {\tt <=}), are very widely used.  Most mutation testing tools parse the code to be mutated, and many do not work on code that does not parse.  However, recently there has been a proposal to perform mutation testing using truly purely syntactic operations, defined by a set of regular expressions implemention operations~\cite{regexpMut}.  Rather than taking a program, per se, this approach simply takes ``code-like'' text and produces a set of variants that, if the original text is compiling source code, will include most common mutations.  The essence of this approach to mutation testing, which can be applied to ``any language,'' is essentially a transformation from arbitrary bytes to arbitrary bytes that, if the original bytes are ``code-like'' will tend to preserve the property of being ``code-like.''


\subsection{Combining Both Forms of Mutation}

Our approach is, in essence, simple.  We add a set of mutations to the repertoire of a mutation-based fuzzer, for use in compiler fuzzing.  These mutations are either traditional mutation operators from the mutation testing domain or inspired by traditional mutation operators, but with changes made to satisfy the needs of fuzzing.  The key point is that, unlike most changes made by mutation-based fuzzers, these mutations are likely to take interesting code inputs and preserve the property, e.g., that the input will get through a parser or trigger interesting optimizations.  The tendency to preserve such properties is natural, since the basis of mutation testing is to take an existing program and produce a set of new, similar programs, by applying mutation operators.  If most mutation operators tended to produce uninteresting code that doesn't even compile, mutation testing would not be of use to anyone.  Moreover, because our approach is based on the idea of a ``universal'' mutation tool~\cite{regexpMut}, the mutation operators used are generally language-agnostic, and useful for fuzzing any programming language that syntactically resembles common languages (under which we include not only C-like languages, but even LISP-like languages based on s-expressions).

\subsection{Limitations}

The most important limitation for the mutation-testing-based approach is that if compiler \emph{crashes} are mostly uninteresting, fuzzing of this kind will probably not be very useful.  This applies, of course, to all AFL-style fuzzing, not just to fuzzing using the technique proposed in this paper.  For example, C and C++ include a large variety of undefined behaviors.  Code that crashes a C or C++ compiler, but that includes (unusual) undefined behavior may well be ignored by developers.  Csmith~\cite{csmith} devotes a great deal of effort to avoiding generating code that falls outside the ``ineresting'' part of the language.  On the other hand, many languages more recent than C and C++ attempt to provide a more ``total'' language where, while a program may be considered absurd by a human, fewer (or no) programs are undefined in the sense that C and C++ use the term.  For example, smart contract languages such as those studied in this paper, generally aim to make all programs that compile well-defined, or at least minimize the problem to more managable cases such as order of evaluation of sub-expressions.  Similarly, Rust code without use of {\tt unsafe} should not crash the compiler, and any such crashes indicate possible bugs in the Rust compiler or type system.  For most more recent languages, and some older languages such as Java, a program that crashes the compiler is, in general, likely of interest to compiler developers.  However, the proposed technique will be much more limited in effectiveness for C and C++ compilers.